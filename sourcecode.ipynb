{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Exploring Audio Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import librosa  # Library for audio and music processing\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import os  # Library for interacting with the operating system\n",
    "\n",
    "# Define the path to the audio dataset\n",
    "audio_dataset_path = 'UrbanSound8K/audio'\n",
    "\n",
    "# Load the metadata from the CSV file\n",
    "metadata = pd.read_csv(\"UrbanSound8K/metadata/UrbanSound8k.csv\")\n",
    "\n",
    "# Display the first 10 rows of the metadata dataframe\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting MFCC Features from Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary library\n",
    "import numpy as np  # Library for numerical operations\n",
    "\n",
    "# Define a function to extract features from an audio file\n",
    "def feature_extractor(file):\n",
    "    # Load the audio file with a specific sample rate conversion\n",
    "    audio, sample_rate = librosa.load(file, res_type='kaiser_fast') \n",
    "    \n",
    "    # Extract MFCC (Mel-frequency cepstral coefficients) features from the audio\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    \n",
    "    # Scale the MFCC features by taking the mean across the time axis\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    \n",
    "    # Return the scaled MFCC features\n",
    "    return mfccs_scaled_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Features from Audio Dataset with Progress Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3554it [04:08, 23.04it/s]c:\\Users\\DNR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8324it [07:36, 27.44it/s]c:\\Users\\DNR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "c:\\Users\\DNR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [07:54, 18.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import the tqdm library for displaying a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty list to store the extracted features\n",
    "extracted_features = []\n",
    "\n",
    "# Iterate over each row in the metadata dataframe with a progress bar\n",
    "for index_num, row in tqdm(metadata.iterrows()):\n",
    "    # Construct the file path for each audio file\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path), 'fold' + str(row[\"fold\"]), str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    # Get the class label for the current audio file\n",
    "    final_class_labels = row['class']\n",
    "    \n",
    "    # Extract features from the audio file using the feature_extractor function\n",
    "    data = feature_extractor(file_name)\n",
    "    \n",
    "    # Append the extracted features and the class label to the list\n",
    "    extracted_features.append([data, final_class_labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Extracted Audio Features to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-217.35526, 70.22338, -130.38527, -53.282898,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.09818, 109.34077, -52.919525, 60.86475, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-458.79114, 121.38419, -46.520657, 52.00812, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-413.89984, 101.66373, -35.42945, 53.036358, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-446.60352, 113.68541, -52.402214, 60.302044,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-446.8255, 117.011925, -33.7923, 55.406204, 2...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-476.60767, 119.41842, -28.514032, 55.966988,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-464.08258, 116.3101, -28.82692, 49.44204, -4...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-471.3208, 125.25887, -36.935387, 57.428547, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-196.822, 113.993126, -13.813408, 0.40220967,...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-217.35526, 70.22338, -130.38527, -53.282898,...          dog_bark\n",
       "1  [-424.09818, 109.34077, -52.919525, 60.86475, ...  children_playing\n",
       "2  [-458.79114, 121.38419, -46.520657, 52.00812, ...  children_playing\n",
       "3  [-413.89984, 101.66373, -35.42945, 53.036358, ...  children_playing\n",
       "4  [-446.60352, 113.68541, -52.402214, 60.302044,...  children_playing\n",
       "5  [-446.8255, 117.011925, -33.7923, 55.406204, 2...  children_playing\n",
       "6  [-476.60767, 119.41842, -28.514032, 55.966988,...  children_playing\n",
       "7  [-464.08258, 116.3101, -28.82692, 49.44204, -4...  children_playing\n",
       "8  [-471.3208, 125.25887, -36.935387, 57.428547, ...  children_playing\n",
       "9  [-196.822, 113.993126, -13.813408, 0.40220967,...          car_horn"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Converting Extracted Features to Pandas DataFrame ###\n",
    "\n",
    "# Convert the list of extracted features into a pandas DataFrame\n",
    "extracted_features_df = pd.DataFrame(extracted_features, columns=['feature', 'class'])\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "extracted_features_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8732, 40)\n",
      "(8732,)\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset into Features and Labels\n",
    "\n",
    "# Convert the 'feature' column into a numpy array\n",
    "x = np.array(extracted_features_df['feature'].tolist())\n",
    "\n",
    "# Convert the 'class' column into a numpy array\n",
    "y = np.array(extracted_features_df['class'].tolist())\n",
    "\n",
    "# Print the shape of the features array\n",
    "print(x.shape)\n",
    "\n",
    "# Print the shape of the labels array\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Class Labels to One-Hot Encoding\n",
    "\n",
    "# Convert the class labels into one-hot encoded format\n",
    "y = np.array(pd.get_dummies(y))\n",
    "\n",
    "# Print the shape of the one-hot encoded labels array\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6985, 40)\n",
      "(6985, 10)\n",
      "(1747, 40)\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# Import the train_test_split function from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Print the shape of the training features array\n",
    "print(x_train.shape)\n",
    "\n",
    "# Print the shape of the training labels array\n",
    "print(y_train.shape)\n",
    "\n",
    "# Print the shape of the testing features array\n",
    "print(x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## MODEL BUILDING ###############################\n",
    "\n",
    "# Import necessary libraries for building the neural network model\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from tensorflow.keras.models import Sequential  # For creating a linear stack of layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten  # Core layers for the model\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizer for compiling the model\n",
    "from sklearn import metrics  # For model evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the Number of Output Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Determine the Number of Output Labels\n",
    "\n",
    "# Calculate the number of output labels based on the shape of the y array (one-hot encoded labels)\n",
    "num_labels = y.shape[1]\n",
    "\n",
    "# Print the number of output labels\n",
    "print(num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Sequential Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "\n",
    "# Layer 1\n",
    "model.add(Dense(100, input_shape=(40,)))  # Add a Dense layer with 100 units and input shape of (40,)\n",
    "model.add(Activation('relu'))  # Apply ReLU activation function\n",
    "model.add(Dropout(0.25))  # Apply dropout with a rate of 25%\n",
    "\n",
    "# Layer 2\n",
    "model.add(Dense(1000))  # Add a Dense layer with 1000 units\n",
    "model.add(Activation('relu'))  # Apply ReLU activation function\n",
    "model.add(Dropout(0.01))  # Apply dropout with a rate of 1%\n",
    "\n",
    "# Layer 3\n",
    "model.add(Dense(500))  # Add a Dense layer with 500 units\n",
    "model.add(Activation('relu'))  # Apply ReLU activation function\n",
    "model.add(Dropout(0.01))  # Apply dropout with a rate of 1%\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_labels))  # Add a Dense layer with the number of output labels\n",
    "model.add(Activation('softmax'))  # Apply softmax activation for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">101,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,010</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m4,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │       \u001b[38;5;34m101,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │       \u001b[38;5;34m500,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,010\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">610,610</span> (2.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m610,610\u001b[0m (2.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">610,610</span> (2.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m610,610\u001b[0m (2.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "\n",
    "# Compile the model with categorical crossentropy loss, accuracy metric, and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2946 - loss: 5.6119\n",
      "Epoch 1: val_loss improved from inf to 1.47994, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2980 - loss: 5.4708 - val_accuracy: 0.4585 - val_loss: 1.4799\n",
      "Epoch 2/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4872 - loss: 1.4496\n",
      "Epoch 2: val_loss improved from 1.47994 to 1.13300, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4885 - loss: 1.4471 - val_accuracy: 0.6348 - val_loss: 1.1330\n",
      "Epoch 3/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5729 - loss: 1.2243\n",
      "Epoch 3: val_loss improved from 1.13300 to 1.02770, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5742 - loss: 1.2219 - val_accuracy: 0.6703 - val_loss: 1.0277\n",
      "Epoch 4/100\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6186 - loss: 1.0803\n",
      "Epoch 4: val_loss improved from 1.02770 to 0.93353, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6192 - loss: 1.0794 - val_accuracy: 0.7052 - val_loss: 0.9335\n",
      "Epoch 5/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6716 - loss: 0.9505\n",
      "Epoch 5: val_loss improved from 0.93353 to 0.86529, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6719 - loss: 0.9500 - val_accuracy: 0.7281 - val_loss: 0.8653\n",
      "Epoch 6/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6968 - loss: 0.8869\n",
      "Epoch 6: val_loss improved from 0.86529 to 0.75814, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6976 - loss: 0.8846 - val_accuracy: 0.7539 - val_loss: 0.7581\n",
      "Epoch 7/100\n",
      "\u001b[1m195/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7331 - loss: 0.7798\n",
      "Epoch 7: val_loss improved from 0.75814 to 0.72684, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7327 - loss: 0.7800 - val_accuracy: 0.7722 - val_loss: 0.7268\n",
      "Epoch 8/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7512 - loss: 0.7159\n",
      "Epoch 8: val_loss improved from 0.72684 to 0.62469, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7515 - loss: 0.7157 - val_accuracy: 0.7962 - val_loss: 0.6247\n",
      "Epoch 9/100\n",
      "\u001b[1m213/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7673 - loss: 0.6548\n",
      "Epoch 9: val_loss improved from 0.62469 to 0.58501, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7674 - loss: 0.6549 - val_accuracy: 0.8266 - val_loss: 0.5850\n",
      "Epoch 10/100\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7827 - loss: 0.6105\n",
      "Epoch 10: val_loss improved from 0.58501 to 0.52895, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7829 - loss: 0.6104 - val_accuracy: 0.8403 - val_loss: 0.5289\n",
      "Epoch 11/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.5771\n",
      "Epoch 11: val_loss improved from 0.52895 to 0.49258, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8061 - loss: 0.5754 - val_accuracy: 0.8351 - val_loss: 0.4926\n",
      "Epoch 12/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8216 - loss: 0.5229\n",
      "Epoch 12: val_loss improved from 0.49258 to 0.46957, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8210 - loss: 0.5246 - val_accuracy: 0.8500 - val_loss: 0.4696\n",
      "Epoch 13/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.5001\n",
      "Epoch 13: val_loss improved from 0.46957 to 0.45857, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8224 - loss: 0.5003 - val_accuracy: 0.8655 - val_loss: 0.4586\n",
      "Epoch 14/100\n",
      "\u001b[1m194/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.4804\n",
      "Epoch 14: val_loss improved from 0.45857 to 0.45321, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8280 - loss: 0.4807 - val_accuracy: 0.8592 - val_loss: 0.4532\n",
      "Epoch 15/100\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.4511\n",
      "Epoch 15: val_loss improved from 0.45321 to 0.41758, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8448 - loss: 0.4512 - val_accuracy: 0.8655 - val_loss: 0.4176\n",
      "Epoch 16/100\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8573 - loss: 0.4312\n",
      "Epoch 16: val_loss did not improve from 0.41758\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8573 - loss: 0.4299 - val_accuracy: 0.8683 - val_loss: 0.4191\n",
      "Epoch 17/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8482 - loss: 0.4362\n",
      "Epoch 17: val_loss improved from 0.41758 to 0.40763, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8483 - loss: 0.4358 - val_accuracy: 0.8689 - val_loss: 0.4076\n",
      "Epoch 18/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3919\n",
      "Epoch 18: val_loss did not improve from 0.40763\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8678 - loss: 0.3917 - val_accuracy: 0.8781 - val_loss: 0.4121\n",
      "Epoch 19/100\n",
      "\u001b[1m193/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3630\n",
      "Epoch 19: val_loss improved from 0.40763 to 0.38459, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8745 - loss: 0.3638 - val_accuracy: 0.8786 - val_loss: 0.3846\n",
      "Epoch 20/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8666 - loss: 0.3856\n",
      "Epoch 20: val_loss improved from 0.38459 to 0.37003, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8664 - loss: 0.3860 - val_accuracy: 0.8867 - val_loss: 0.3700\n",
      "Epoch 21/100\n",
      "\u001b[1m213/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.3364\n",
      "Epoch 21: val_loss improved from 0.37003 to 0.34178, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8829 - loss: 0.3364 - val_accuracy: 0.8924 - val_loss: 0.3418\n",
      "Epoch 22/100\n",
      "\u001b[1m195/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8878 - loss: 0.3357\n",
      "Epoch 22: val_loss did not improve from 0.34178\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.3374 - val_accuracy: 0.8970 - val_loss: 0.3540\n",
      "Epoch 23/100\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3625\n",
      "Epoch 23: val_loss did not improve from 0.34178\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.3620 - val_accuracy: 0.8964 - val_loss: 0.3527\n",
      "Epoch 24/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8864 - loss: 0.3214\n",
      "Epoch 24: val_loss did not improve from 0.34178\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8864 - loss: 0.3215 - val_accuracy: 0.8964 - val_loss: 0.3642\n",
      "Epoch 25/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8861 - loss: 0.3199\n",
      "Epoch 25: val_loss improved from 0.34178 to 0.33042, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8862 - loss: 0.3195 - val_accuracy: 0.8998 - val_loss: 0.3304\n",
      "Epoch 26/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9004 - loss: 0.2865\n",
      "Epoch 26: val_loss did not improve from 0.33042\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9002 - loss: 0.2865 - val_accuracy: 0.8918 - val_loss: 0.3588\n",
      "Epoch 27/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8891 - loss: 0.3197\n",
      "Epoch 27: val_loss did not improve from 0.33042\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8892 - loss: 0.3193 - val_accuracy: 0.9021 - val_loss: 0.3525\n",
      "Epoch 28/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.2890\n",
      "Epoch 28: val_loss did not improve from 0.33042\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.2884 - val_accuracy: 0.8993 - val_loss: 0.3453\n",
      "Epoch 29/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.2626\n",
      "Epoch 29: val_loss improved from 0.33042 to 0.32769, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9111 - loss: 0.2633 - val_accuracy: 0.9061 - val_loss: 0.3277\n",
      "Epoch 30/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9076 - loss: 0.2703\n",
      "Epoch 30: val_loss did not improve from 0.32769\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9076 - loss: 0.2704 - val_accuracy: 0.8998 - val_loss: 0.3308\n",
      "Epoch 31/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9158 - loss: 0.2498\n",
      "Epoch 31: val_loss did not improve from 0.32769\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9153 - loss: 0.2509 - val_accuracy: 0.9015 - val_loss: 0.3355\n",
      "Epoch 32/100\n",
      "\u001b[1m193/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9043 - loss: 0.2756\n",
      "Epoch 32: val_loss did not improve from 0.32769\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9049 - loss: 0.2750 - val_accuracy: 0.9021 - val_loss: 0.3315\n",
      "Epoch 33/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9217 - loss: 0.2181\n",
      "Epoch 33: val_loss improved from 0.32769 to 0.28932, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9216 - loss: 0.2187 - val_accuracy: 0.9204 - val_loss: 0.2893\n",
      "Epoch 34/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9173 - loss: 0.2375\n",
      "Epoch 34: val_loss did not improve from 0.28932\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9173 - loss: 0.2376 - val_accuracy: 0.9124 - val_loss: 0.3479\n",
      "Epoch 35/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9119 - loss: 0.2559\n",
      "Epoch 35: val_loss did not improve from 0.28932\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9119 - loss: 0.2559 - val_accuracy: 0.8964 - val_loss: 0.3428\n",
      "Epoch 36/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.2499\n",
      "Epoch 36: val_loss did not improve from 0.28932\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9166 - loss: 0.2500 - val_accuracy: 0.9015 - val_loss: 0.3434\n",
      "Epoch 37/100\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9289 - loss: 0.2092\n",
      "Epoch 37: val_loss did not improve from 0.28932\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9281 - loss: 0.2116 - val_accuracy: 0.9113 - val_loss: 0.3214\n",
      "Epoch 38/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9212 - loss: 0.2513\n",
      "Epoch 38: val_loss did not improve from 0.28932\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9209 - loss: 0.2513 - val_accuracy: 0.9153 - val_loss: 0.3221\n",
      "Epoch 39/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9144 - loss: 0.2592\n",
      "Epoch 39: val_loss improved from 0.28932 to 0.28770, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9144 - loss: 0.2590 - val_accuracy: 0.9204 - val_loss: 0.2877\n",
      "Epoch 40/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 0.2078\n",
      "Epoch 40: val_loss did not improve from 0.28770\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.2085 - val_accuracy: 0.9090 - val_loss: 0.2945\n",
      "Epoch 41/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9327 - loss: 0.2037\n",
      "Epoch 41: val_loss improved from 0.28770 to 0.28259, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9326 - loss: 0.2038 - val_accuracy: 0.9262 - val_loss: 0.2826\n",
      "Epoch 42/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9307 - loss: 0.2103\n",
      "Epoch 42: val_loss did not improve from 0.28259\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9306 - loss: 0.2111 - val_accuracy: 0.9153 - val_loss: 0.3280\n",
      "Epoch 43/100\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9358 - loss: 0.1830\n",
      "Epoch 43: val_loss did not improve from 0.28259\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9357 - loss: 0.1835 - val_accuracy: 0.9227 - val_loss: 0.3008\n",
      "Epoch 44/100\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1870\n",
      "Epoch 44: val_loss did not improve from 0.28259\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9346 - loss: 0.1887 - val_accuracy: 0.9284 - val_loss: 0.3060\n",
      "Epoch 45/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9266 - loss: 0.2146\n",
      "Epoch 45: val_loss did not improve from 0.28259\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9266 - loss: 0.2146 - val_accuracy: 0.9170 - val_loss: 0.3256\n",
      "Epoch 46/100\n",
      "\u001b[1m192/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9183 - loss: 0.2397\n",
      "Epoch 46: val_loss improved from 0.28259 to 0.27052, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9193 - loss: 0.2369 - val_accuracy: 0.9284 - val_loss: 0.2705\n",
      "Epoch 47/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2083\n",
      "Epoch 47: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2082 - val_accuracy: 0.9130 - val_loss: 0.3056\n",
      "Epoch 48/100\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9291 - loss: 0.1983\n",
      "Epoch 48: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9291 - loss: 0.1984 - val_accuracy: 0.9159 - val_loss: 0.3024\n",
      "Epoch 49/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9368 - loss: 0.1911\n",
      "Epoch 49: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9366 - loss: 0.1915 - val_accuracy: 0.9170 - val_loss: 0.2932\n",
      "Epoch 50/100\n",
      "\u001b[1m196/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9379 - loss: 0.1918\n",
      "Epoch 50: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9378 - loss: 0.1919 - val_accuracy: 0.9159 - val_loss: 0.3001\n",
      "Epoch 51/100\n",
      "\u001b[1m195/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9412 - loss: 0.1778\n",
      "Epoch 51: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9405 - loss: 0.1796 - val_accuracy: 0.9050 - val_loss: 0.3731\n",
      "Epoch 52/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.2017\n",
      "Epoch 52: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9337 - loss: 0.2008 - val_accuracy: 0.9204 - val_loss: 0.3243\n",
      "Epoch 53/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9406 - loss: 0.1699\n",
      "Epoch 53: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9403 - loss: 0.1716 - val_accuracy: 0.9296 - val_loss: 0.2868\n",
      "Epoch 54/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.1770\n",
      "Epoch 54: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9422 - loss: 0.1770 - val_accuracy: 0.9210 - val_loss: 0.3162\n",
      "Epoch 55/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1908\n",
      "Epoch 55: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1907 - val_accuracy: 0.9239 - val_loss: 0.3052\n",
      "Epoch 56/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9404 - loss: 0.1872\n",
      "Epoch 56: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9406 - loss: 0.1860 - val_accuracy: 0.9187 - val_loss: 0.3329\n",
      "Epoch 57/100\n",
      "\u001b[1m213/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9392 - loss: 0.1900\n",
      "Epoch 57: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9391 - loss: 0.1899 - val_accuracy: 0.9319 - val_loss: 0.2804\n",
      "Epoch 58/100\n",
      "\u001b[1m196/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.1496\n",
      "Epoch 58: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1512 - val_accuracy: 0.9233 - val_loss: 0.3305\n",
      "Epoch 59/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 0.2046\n",
      "Epoch 59: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9320 - loss: 0.2045 - val_accuracy: 0.9313 - val_loss: 0.2890\n",
      "Epoch 60/100\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.1995\n",
      "Epoch 60: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9337 - loss: 0.1990 - val_accuracy: 0.9244 - val_loss: 0.3223\n",
      "Epoch 61/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9402 - loss: 0.1695\n",
      "Epoch 61: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9402 - loss: 0.1697 - val_accuracy: 0.9244 - val_loss: 0.3109\n",
      "Epoch 62/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9473 - loss: 0.1633\n",
      "Epoch 62: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.1635 - val_accuracy: 0.9170 - val_loss: 0.3678\n",
      "Epoch 63/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9436 - loss: 0.1642\n",
      "Epoch 63: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.1647 - val_accuracy: 0.9233 - val_loss: 0.3124\n",
      "Epoch 64/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1772\n",
      "Epoch 64: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1771 - val_accuracy: 0.9284 - val_loss: 0.3146\n",
      "Epoch 65/100\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1643\n",
      "Epoch 65: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9466 - loss: 0.1653 - val_accuracy: 0.9250 - val_loss: 0.3145\n",
      "Epoch 66/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1752\n",
      "Epoch 66: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1756 - val_accuracy: 0.9267 - val_loss: 0.3196\n",
      "Epoch 67/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9534 - loss: 0.1591\n",
      "Epoch 67: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9532 - loss: 0.1591 - val_accuracy: 0.9244 - val_loss: 0.3342\n",
      "Epoch 68/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9482 - loss: 0.1500\n",
      "Epoch 68: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1500 - val_accuracy: 0.9210 - val_loss: 0.3599\n",
      "Epoch 69/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9499 - loss: 0.1622\n",
      "Epoch 69: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9498 - loss: 0.1622 - val_accuracy: 0.9227 - val_loss: 0.3240\n",
      "Epoch 70/100\n",
      "\u001b[1m189/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9492 - loss: 0.1637\n",
      "Epoch 70: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9498 - loss: 0.1619 - val_accuracy: 0.9347 - val_loss: 0.3241\n",
      "Epoch 71/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9527 - loss: 0.1420\n",
      "Epoch 71: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9524 - loss: 0.1426 - val_accuracy: 0.9290 - val_loss: 0.3334\n",
      "Epoch 72/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9447 - loss: 0.1687\n",
      "Epoch 72: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9444 - loss: 0.1693 - val_accuracy: 0.9273 - val_loss: 0.3179\n",
      "Epoch 73/100\n",
      "\u001b[1m189/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1376\n",
      "Epoch 73: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.1390 - val_accuracy: 0.9250 - val_loss: 0.3132\n",
      "Epoch 74/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.1564\n",
      "Epoch 74: val_loss did not improve from 0.27052\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9519 - loss: 0.1570 - val_accuracy: 0.9199 - val_loss: 0.3416\n",
      "Epoch 75/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9462 - loss: 0.1562\n",
      "Epoch 75: val_loss improved from 0.27052 to 0.26019, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9461 - loss: 0.1574 - val_accuracy: 0.9365 - val_loss: 0.2602\n",
      "Epoch 76/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9466 - loss: 0.1614\n",
      "Epoch 76: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.1616 - val_accuracy: 0.9319 - val_loss: 0.2792\n",
      "Epoch 77/100\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9471 - loss: 0.1539\n",
      "Epoch 77: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9473 - loss: 0.1537 - val_accuracy: 0.9222 - val_loss: 0.3310\n",
      "Epoch 78/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9594 - loss: 0.1401\n",
      "Epoch 78: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1406 - val_accuracy: 0.9250 - val_loss: 0.3081\n",
      "Epoch 79/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9464 - loss: 0.1568\n",
      "Epoch 79: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9466 - loss: 0.1565 - val_accuracy: 0.9233 - val_loss: 0.3172\n",
      "Epoch 80/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9459 - loss: 0.1896\n",
      "Epoch 80: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9461 - loss: 0.1883 - val_accuracy: 0.9239 - val_loss: 0.3131\n",
      "Epoch 81/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.1449\n",
      "Epoch 81: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9555 - loss: 0.1450 - val_accuracy: 0.9233 - val_loss: 0.3367\n",
      "Epoch 82/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1444\n",
      "Epoch 82: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.1448 - val_accuracy: 0.9199 - val_loss: 0.3394\n",
      "Epoch 83/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9459 - loss: 0.1689\n",
      "Epoch 83: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9463 - loss: 0.1680 - val_accuracy: 0.9353 - val_loss: 0.2813\n",
      "Epoch 84/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.1415\n",
      "Epoch 84: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9490 - loss: 0.1411 - val_accuracy: 0.9296 - val_loss: 0.3059\n",
      "Epoch 85/100\n",
      "\u001b[1m194/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9527 - loss: 0.1444\n",
      "Epoch 85: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9528 - loss: 0.1453 - val_accuracy: 0.9313 - val_loss: 0.2884\n",
      "Epoch 86/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1300\n",
      "Epoch 86: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1301 - val_accuracy: 0.9267 - val_loss: 0.3149\n",
      "Epoch 87/100\n",
      "\u001b[1m194/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.1453\n",
      "Epoch 87: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9564 - loss: 0.1467 - val_accuracy: 0.9307 - val_loss: 0.2961\n",
      "Epoch 88/100\n",
      "\u001b[1m193/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1555\n",
      "Epoch 88: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9537 - loss: 0.1555 - val_accuracy: 0.9153 - val_loss: 0.3704\n",
      "Epoch 89/100\n",
      "\u001b[1m189/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1482\n",
      "Epoch 89: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9510 - loss: 0.1462 - val_accuracy: 0.9267 - val_loss: 0.3103\n",
      "Epoch 90/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9513 - loss: 0.1556\n",
      "Epoch 90: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9513 - loss: 0.1555 - val_accuracy: 0.9239 - val_loss: 0.3261\n",
      "Epoch 91/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.1378\n",
      "Epoch 91: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9555 - loss: 0.1379 - val_accuracy: 0.9267 - val_loss: 0.3119\n",
      "Epoch 92/100\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1565\n",
      "Epoch 92: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1566 - val_accuracy: 0.9227 - val_loss: 0.2899\n",
      "Epoch 93/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.1369\n",
      "Epoch 93: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1360 - val_accuracy: 0.9256 - val_loss: 0.3100\n",
      "Epoch 94/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9629 - loss: 0.1338\n",
      "Epoch 94: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9628 - loss: 0.1335 - val_accuracy: 0.9319 - val_loss: 0.2914\n",
      "Epoch 95/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1349\n",
      "Epoch 95: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1347 - val_accuracy: 0.9313 - val_loss: 0.3057\n",
      "Epoch 96/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9556 - loss: 0.1578\n",
      "Epoch 96: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9556 - loss: 0.1575 - val_accuracy: 0.9284 - val_loss: 0.2760\n",
      "Epoch 97/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9629 - loss: 0.1142\n",
      "Epoch 97: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1153 - val_accuracy: 0.9262 - val_loss: 0.3337\n",
      "Epoch 98/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.1161\n",
      "Epoch 98: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.1179 - val_accuracy: 0.9244 - val_loss: 0.3336\n",
      "Epoch 99/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1453\n",
      "Epoch 99: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1453 - val_accuracy: 0.9330 - val_loss: 0.3457\n",
      "Epoch 100/100\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1410\n",
      "Epoch 100: val_loss did not improve from 0.26019\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 0.1403 - val_accuracy: 0.9262 - val_loss: 0.3220\n",
      "Training completed in time: 0:00:56.729802\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Define the number of epochs and batch size for training\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model during training\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.keras', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# Record the start time for training duration calculation\n",
    "start = datetime.now()\n",
    "\n",
    "# Train the model using the defined parameters and callbacks\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \n",
    "          validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "# Calculate and print the training duration\n",
    "duration = datetime.now() - start\n",
    "print('Training completed in time:', duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32199984788894653, 0.926159143447876]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on Test Data\n",
    "\n",
    "# Evaluate the model on the test data to get the test accuracy\n",
    "test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUrbanSound8K\u001b[39m\u001b[38;5;130;01m\\a\u001b[39;00m\u001b[38;5;124mudio\u001b[39m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124mold3\u001b[39m\u001b[38;5;130;01m\\176\u001b[39;00m\u001b[38;5;124m15-3-0-4.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract features from the audio file for prediction\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m prediction_feature \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m(filename)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Reshape the extracted feature for prediction\u001b[39;00m\n\u001b[0;32m      8\u001b[0m prediction_feature \u001b[38;5;241m=\u001b[39m prediction_feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the filename of the audio file for prediction\n",
    "filename = 'UrbanSound8K\\audio\\fold3\\17615-3-0-4.wav'\n",
    "\n",
    "# Extract features from the audio file for prediction\n",
    "prediction_feature = feature_extractor(filename)\n",
    "\n",
    "# Reshape the extracted feature for prediction\n",
    "prediction_feature = prediction_feature.reshape(1, -1)\n",
    "\n",
    "# Predict the class probabilities for the input feature\n",
    "prediction_probabilities = model.predict(prediction_feature)\n",
    "\n",
    "# Determine the predicted class based on the highest probability\n",
    "predicted_class = prediction_probabilities.argmax(axis=-1)\n",
    "\n",
    "# Print the predicted class\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog_bark' 'children_playing' 'car_horn' 'air_conditioner' 'street_music'\n",
      " 'gun_shot' 'siren' 'engine_idling' 'jackhammer' 'drilling']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values from the 'class' column in the metadata DataFrame\n",
    "unique_classes = metadata['class'].unique()\n",
    "\n",
    "# Print the unique classes\n",
    "print(unique_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding and Decoding for Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog_bark']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Get unique class names from the metadata\n",
    "class_names = metadata['class'].unique()\n",
    "\n",
    "# Create and fit the LabelEncoder to map class names to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(class_names)\n",
    "\n",
    "# Inverse transform the predicted class label to get the predicted class name\n",
    "predicted_class_name = label_encoder.inverse_transform(predicted_class)\n",
    "\n",
    "# Print the predicted class name\n",
    "print(predicted_class_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
